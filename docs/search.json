[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Simítás, spline-ok, additív modellek",
    "section": "",
    "text": "Előszó\nEz a jegyzet simítóeljárásokkal (pl. LOESS), a spline-okkal, és azok regresszióban történő felhasználásával, valamint általában az additív modellekkel foglalkozik.\nAjánlott irodalom: Simon N. Wood: Generalized Additive Models: an introduction with R (Chapman & Hall/CRC, Texts in Statistical Science sorozat, 2. kiadás, ISBN 9781498728331, 2017).\nA jegyzettel kapcsolatban minden visszajelzést, véleményt, kritikát a lehető legnagyobb örömmel veszek a tamas.ferenci@medstat.hu email-címen.\nA jegyzet weboldala https://github.com/ferenci-tamas/simitas-spline-additiv-modell címen érhető el.\nEhhez az anyaghoz csak a data.table és a ggplot2 könyvtárakra lesz szükségünk (illetve beállítjuk a véletlenszám-generátor seed-jét1 a reprodukálhatóság kedvéért):\nlibrary(data.table)\nlibrary(ggplot2)\nset.seed(1)",
    "crumbs": [
      "Előszó"
    ]
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Simítás, spline-ok, additív modellek",
    "section": "",
    "text": "A véletlenszám-generátort használó kódok eredménye szükségképp el fog térni bármely két futtatás között, így például az is el fog térni egymástól ami itt látható, és amit valaki kap, ha lefuttatja ugyanezt a kódot. Hogy ezt kiküszöböljük, és az olvasó pontosan ugyanazt az eredményt kapja, érdemes a seed-et beállítani: ezt követően a pontosan ugyanúgy lefuttatott kód ugyanazt az eredményt kell, hogy adja. (Természetesen a véletlenszám-generálásnál kapott számok továbbra is véletlenek lesznek – amennyire eredetileg is azok… – de ugyanabban a sorrendben fognak jönni.)↩︎",
    "crumbs": [
      "Előszó"
    ]
  },
  {
    "objectID": "LOESS.html",
    "href": "LOESS.html",
    "title": "1  A LOESS simító",
    "section": "",
    "text": "1.1 Motiváció\nElső lépésben előkészítünk egy demonstrációs adatbázist. Szimulált adatokat fogunk használni (zajos szinusz), így mi is tudni fogjuk, hogy mi az igazság, a valódi függvény amiből a pontok jöttek:\nn &lt;- 101\nSimData &lt;- data.table(x = (1:n) + rnorm(n, 0, 0.1))\nSimData$y &lt;- sin(SimData$x/n*(2*pi))\nSimData$yobs &lt;- SimData$y + rnorm(n, 0, 0.2)\np &lt;- ggplot(SimData, aes(x = x, y = yobs)) + geom_point() +\n  geom_line(aes(y = y), color = \"orange\", lwd = 1)\np\nParaméteres görbeillesztésnél fel kell tételeznünk egy függvényformát (ti. ami a pontok mögött van a valóságban). Például, hogy lineáris:\np + geom_smooth(formula = y~x, method = \"lm\", se = FALSE)\nEz az ábra jól mutatja ennek a fő problémáját: hogy ezt a feltételezést el is ronthatjuk! Természetesen vannak diagnosztikai eszközök ennek felderítésére, például megnézhetjük a reziduumokat az \\(x\\) függvényében, ami a fenti esetben nagyon csúnyán fog kinézni, és ez alapján kereshetünk jobb függvényformát, de gyökerestül csak az oldja meg a problémát, ha olyan módszert találnánk, ami a nélkül működik, hogy egyáltalán fel kelljen tételeznie – bármilyen – függvényformát. Ezt valósítják meg a simítási eljárások. (Lényegében nemparaméteres regresszióról van szó.)\nAnnak az előnynek, hogy nem kell ilyen feltételezéssel élnünk (és így azt el sem ronthatjuk), természetesen ára van: kevésbé hatásosan becsülhető, mint a paraméteres illesztés, nincsenek számszerű paramétereink (aminek esetleg tárgyterületi interpretációt lehet adni), csak ábrát tudunk rajzolni, és végezetül extrapoláció sem lehetséges, legalábbis nem triviálisan.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>A LOESS simító</span>"
    ]
  },
  {
    "objectID": "LOESS.html#a-loess-simító-alapgondolata",
    "href": "LOESS.html#a-loess-simító-alapgondolata",
    "title": "1  A LOESS simító",
    "section": "1.2 A LOESS simító alapgondolata",
    "text": "1.2 A LOESS simító alapgondolata\nAz egyik legnépszerűbb megoldás a LOESS (locally weighted scatterplot smoothing, néha LOWESS, vagy Savitzky–Golay szűrő), melynek alapgondolata, hogy végigmegy az \\(x\\)-változó releváns tartományán, és minden értékre meghatározza a pontfelhő ottani, tehát lokális közelítését, egy polinomális regresszióból. Utána az egész simítást ezekből a darabkákból építi fel.\nLegyen például a vizsgált érték a \\(23.5\\):\n\np + geom_vline(xintercept = 23.5, color = \"red\")",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>A LOESS simító</span>"
    ]
  },
  {
    "objectID": "LOESS.html#lokalitás",
    "href": "LOESS.html#lokalitás",
    "title": "1  A LOESS simító",
    "section": "1.3 Lokalitás",
    "text": "1.3 Lokalitás\nA lokalitást két eszközzel érjük el. Az egyik, hogy nem használjuk az összes pontot, csak a vizsgált értékhez legközelebb eső \\(\\alpha\\) hányadát (ha ez nem egész lenne, akkor felső egészrészt veszünk); ezt a paramétert szokták span-nek is nevezni. Például, ha ez 75%, akkor a távolság ameddig figyelembe vesszük a pontokat:\n\nspan &lt;- 0.75\nn*span\n\n[1] 75.75\n\nceiling(n*span)\n\n[1] 76\n\nsort(abs(SimData$x-23.5))[ceiling(n*span)]\n\n[1] 52.52914\n\n\nA másik eszköz, hogy még a megtartott pontokon belül is súlyozunk: minél távolabb esik egy pont a vizsgált értéktől annál kisebb lesz a súlya. Általában a trikubikus súlyfüggvényt használjuk:\n\ntricube &lt;- function(u, t) ifelse(u&lt;t, (1-(u/t)^3)^3, 0)\ncurve(tricube(x, 2), to = 3)\n\n\n\n\n\n\n\n\n(A fenti definícióval természetesen a kétféle lépés együtt van benne a tricube függvényben.)\nA felhasznált pontok:\n\nSimData$w &lt;- tricube(abs(SimData$x-23.5), sort(abs(SimData$x-23.5))[ceiling(n*span)])\nggplot(SimData, aes(x = x, y = yobs, color = w&gt;0)) + geom_point()\n\n\n\n\n\n\n\n\nA súlyozás:\n\nggplot(SimData, aes(x = x, y = yobs, color = w)) + geom_point()",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>A LOESS simító</span>"
    ]
  },
  {
    "objectID": "LOESS.html#polinomiális-regresszió",
    "href": "LOESS.html#polinomiális-regresszió",
    "title": "1  A LOESS simító",
    "section": "1.4 Polinomiális regresszió",
    "text": "1.4 Polinomiális regresszió\nA leszűkített és átsúlyozott ponthalmazra – ezt most tehát egyben tartalmazza a w – egy polinomális regressziót illesztünk.\nLegegyszerűbb esetben ez lineáris regresszió:\n\nfit &lt;- lm(yobs ~ x, weights = w, data = SimData)\np + geom_vline(xintercept = 23.5, color = \"red\") +\n  geom_abline(intercept = coef(fit)[1], slope = coef(fit)[2])\n\n\n\n\n\n\n\n\nAz illesztett regressziónak azt a pontját vesszük ki, ami a vizsgált érték volt! Az előbbi példát folytatva:\n\np + geom_abline(intercept = coef(fit)[1], slope = coef(fit)[2]) +\n  geom_vline(xintercept = 23.5, color = \"red\") +\n  geom_point(x = 23.5, y = predict(fit, data.table(x = 23.5)), color=\"red\")\n\n\n\n\n\n\n\n\nA dolgot automatizálhatjuk is:\n\nloessfun &lt;- function(xin, x, yobs, span) {\n  n &lt;- length(x)\n  w &lt;- tricube(abs(x-xin), sort(abs(x-xin))[ceiling(n*span)])\n  fit &lt;- lm(yobs ~ x, weights = w)\n  predict(fit, data.table(x = xin))\n}\np + geom_vline(xintercept = 23.5, color = \"red\") +\n  geom_point(x = 23.5, y = loessfun(23.5, SimData$x, SimData$yobs, 0.75), color=\"red\")\n\n\n\n\n\n\n\n\nEzt használva természetesen kényelmesen kiszámíthatjuk ezt bármely más értékre is:\n\np + geom_vline(xintercept = 48.3, color = \"red\") +\n  geom_point(x = 48.3, y = loessfun(48.3, SimData$x, SimData$yobs, 0.75), color=\"red\")\n\n\n\n\n\n\n\np + geom_vline(xintercept = 91.2, color = \"red\") +\n  geom_point(x = 91.2, y = loessfun(91.2, SimData$x, SimData$yobs, 0.75), color=\"red\")",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>A LOESS simító</span>"
    ]
  },
  {
    "objectID": "LOESS.html#összerakva-az-építőelemeket-lokális-polinomiális-regressziókkal-közelítés",
    "href": "LOESS.html#összerakva-az-építőelemeket-lokális-polinomiális-regressziókkal-közelítés",
    "title": "1  A LOESS simító",
    "section": "1.5 Összerakva az építőelemeket: lokális polinomiális regressziókkal közelítés",
    "text": "1.5 Összerakva az építőelemeket: lokális polinomiális regressziókkal közelítés\nInnen már értelemszerű a következő lépés, számítsuk ki ezeket a simított értékeket az \\(x\\) releváns tartományának minden pontjára:\n\nSmoothData &lt;- data.table(x = seq(0, 101, 0.1))\nSmoothData$value &lt;- apply(SmoothData, 1, function(sm)\n  loessfun(sm[\"x\"], SimData$x, SimData$yobs, 0.75))\np + geom_line(data = SmoothData, aes(x = x, y = value), color = \"red\")\n\n\n\n\n\n\n\n\nEz lesz a LOESS simítás!\nMin múlott az eredmény? Két paramétert használtunk: azt, hogy a pontok mekkora hányadát tartjuk meg, és azt, hogy hányadfokú polinomot illesztettünk. A fenti példában ezek \\(\\alpha=0,\\!75\\) és \\(p=1\\). (Természetesen a súlyozófüggvény a harmadik paraméter, de azt most végig rögzítettnek fogjuk tekinteni.)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>A LOESS simító</span>"
    ]
  },
  {
    "objectID": "LOESS.html#a-paraméterek-megválasztásának-hatása-lokalitás",
    "href": "LOESS.html#a-paraméterek-megválasztásának-hatása-lokalitás",
    "title": "1  A LOESS simító",
    "section": "1.6 A paraméterek megválasztásának hatása: lokalitás",
    "text": "1.6 A paraméterek megválasztásának hatása: lokalitás\nKézenfekvő a kérdés, hogy vajon a simításra hogyan hatnak ezek a paraméterek (annál is inkább, mert a fenti simítás nem néz ki túl bíztatóan!). Kezdjük a lokalitást szabályzó \\(\\alpha\\) paraméter hatásával:\n\nSmoothData &lt;- CJ(x = seq(0, 101, 0.5), span = c(2/n+1e-10, 0.25, 0.5, 0.75, 1))\nSmoothData$value &lt;- apply(SmoothData, 1, function(sm)\n  loessfun(sm[\"x\"], SimData$x, SimData$yobs, sm[\"span\"]))\n\np + geom_line(data = SmoothData[span%in%c(0.25, 0.5, 0.75)],\n              aes(x = x, y = value, color = factor(span)))\n\n\n\n\n\n\n\n\nMég szemléletesebb, ha megnézzük a két szélső értéket is. Ha minden pontot figyelembe veszünk (nincs lokalitás):\n\np + geom_line(data = SmoothData[span==1], aes(x = x, y = value), color = \"red\")\n\n\n\n\n\n\n\n\nHa semennyi pontot nem veszünk figyelembe, a legközelebbi kettő kivételével értelemszerűen, hogy legyen mire illeszteni a görbét (teljes lokalitás):\n\np + geom_line(data = SmoothData[span==2/n+1e-10], aes(x = x, y = value), color = \"red\")\n\n\n\n\n\n\n\n\nAz első esetet szokták úgy hívni, hogy túlsimítás, a másodikat úgy, hogy alulsimítás. Vajon hogyan tudjuk a simítási paraméter (ennél a módszernél az \\(\\alpha\\)) értékét optimálisan megválasztani?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>A LOESS simító</span>"
    ]
  },
  {
    "objectID": "LOESS.html#a-paraméterek-megválasztásának-hatása-a-polinom-fokszáma",
    "href": "LOESS.html#a-paraméterek-megválasztásának-hatása-a-polinom-fokszáma",
    "title": "1  A LOESS simító",
    "section": "1.7 A paraméterek megválasztásának hatása: a polinom fokszáma",
    "text": "1.7 A paraméterek megválasztásának hatása: a polinom fokszáma\nMielőtt az előbbi kérdésre válaszolunk, meg kell nézni még egy kérdést, mert vissza fog hatni a válaszra: az illesztett polinom \\(p\\) fokszámát. Vajon mi történik, ha lineáris regresszió helyett magasabb fokszámú polinomot használunk?\n\n1.7.1 Kitérő: polinomiális regresszió illesztésének szintaktikája R alatt\nÉrdemes kitérni arra a kérdésre, hogy a polinomiális regressziót hogyan kell R alatt specifikálni (az lm-nek megadni).\nA dolognak van ugyanis egy szintaktikai trükkje. Az ugyanis, ami a legkézenfekvőbbnek tűnne, nem működik:\n\nlm(y ~ x + x^2, data = SimData)\n\n\nCall:\nlm(formula = y ~ x + x^2, data = SimData)\n\nCoefficients:\n(Intercept)            x  \n    0.96485     -0.01892  \n\n\nA probléma oka, hogy az lm formula interfészében a műveleti jelek speciálisan viselkednek. A ^ nem a hatványozás jele, hanem interakciót specifikál, azaz az (x+y)^2 ugyanaz mint az x+y+x:y, viszont egy tagnál nincs mivel interakciót képezni (az x:x nem az x saját magával vett szorzata lesz, ami nekünk jó lenne, hanem simán x!), így az x^2 ugyanaz lesz mint az x.\nA megoldást az I() függvény jelenti, ami azt mondja az R-nek, hogy a beleírt kifejezésben szereplő operátorokat a szokásos aritmetikai értelemmel értékelje ki:\n\nlm(y ~ x + I(x^2), data = SimData)\n\n\nCall:\nlm(formula = y ~ x + I(x^2), data = SimData)\n\nCoefficients:\n(Intercept)            x       I(x^2)  \n  1.014e+00   -2.178e-02    2.806e-05  \n\n\nEz már működik, de eljárhatunk egyszerűbben is, a poly függvény ugyanis pont erre szolgál:\n\nlm(y ~ poly(x, 2), data = SimData)\n\n\nCall:\nlm(formula = y ~ poly(x, 2), data = SimData)\n\nCoefficients:\n(Intercept)  poly(x, 2)1  poly(x, 2)2  \n -0.0001279   -5.5423654    0.2142741  \n\n\nLátszólag mást kaptunk, de valójában csak a parametrizálásban van eltérés, a predikciók azonosak:\n\npredict(lm(y ~ x + I(x^2), data = SimData), data.table(x = 43.9))\n\n        1 \n0.1119441 \n\npredict(lm(y ~ poly(x, 2), data = SimData), data.table(x = 43.9))\n\n        1 \n0.1119441 \n\n\nA magyarázat, hogy a poly alapjáraton ortogonalizálja a tagokat (azaz olyan másodfokú polinomot szolgáltat, melynek elemei korrelálatlanok egymással). Nézzük is meg, a kapott vektorok csakugyan ortogonálisak, sőt, sortonormáltak:\n\nt(cbind(1, poly(SimData$x, 3)))%*%cbind(1, poly(SimData$x, 3))\n\n                           1             2             3\n  1.010000e+02  2.498002e-16  2.470246e-15  1.609823e-15\n1 2.498002e-16  1.000000e+00 -3.191891e-16 -5.551115e-17\n2 2.470246e-15 -3.191891e-16  1.000000e+00  1.249001e-16\n3 1.609823e-15 -5.551115e-17  1.249001e-16  1.000000e+00\n\n\nHa szeretnénk, ezt kikapcsolhatjuk, és akkor visszakapjuk a kézel létrehozott eredményt:\n\nlm(y ~ poly(x, 2, raw = TRUE), data = SimData)\n\n\nCall:\nlm(formula = y ~ poly(x, 2, raw = TRUE), data = SimData)\n\nCoefficients:\n            (Intercept)  poly(x, 2, raw = TRUE)1  poly(x, 2, raw = TRUE)2  \n              1.014e+00               -2.178e-02                2.806e-05  \n\n\nAz alapértelmezett persze nem véletlenül az, ami: az ortogonális polinomok becslése sokkal jobb numerikus szempontból. Például a modellmátrix kondíciószámát nézve:\n\nkappa(cbind(1, poly(SimData$x, 3)), exact = TRUE)\n\n[1] 10.04988\n\nkappa(cbind(1, poly(SimData$x, 3, raw = TRUE)), exact = TRUE)\n\n[1] 1651776\n\n\nA poly használata nem csak elegánsabb és numerikusan szerencsésebb, de jóval kényelmesebb is (gondoljunk bele mi volna, ha véletlenül tizedfokú polinomot akarnánk specifikálni, vagy változó lenne, hogy hányadfokú polinomról van szó).\n\n\n1.7.2 Polinom fokszámának változtatása\nMost már könnyedén megoldhatjuk, hogy a fokszám is változtatható legyen:\n\nloessfun &lt;- function(xin, x, yobs, span, degree) {\n  n &lt;- length(x)\n  w &lt;- tricube(abs(x-xin), sort(abs(x-xin))[ceiling(n*span)])\n  fit &lt;- lm(yobs ~ poly(x, degree), weights = w)\n  predict(fit, data.table(x = xin))\n}\n\nEzt használva immár különböző fokszámokkal és simítási paraméterrel is próbálkozhatunk:\n\nSmoothData &lt;- CJ(x = seq(0, 101, 0.5), degree = c(1, 2),\n                 span = (5:99)/100)\nSmoothData$value &lt;- apply(SmoothData, 1, function(sm)\n  loessfun(sm[\"x\"], SimData$x, SimData$yobs, sm[\"span\"], sm[\"degree\"]))\np + geom_line(data = SmoothData[span%in%c(0.25, 0.5, 0.75)],\n              aes(x = x, y = value, color = factor(span))) +\n  facet_grid(rows = vars(degree))\n\n\n\n\n\n\n\n\nEgy nagyon fontos dolgot látunk: ha áttérünk a másodfokú polinom használatára, akkor gyakorlatilag a simítási paramétertől függetlenül szinte tökéletes simítást kapunk!",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>A LOESS simító</span>"
    ]
  },
  {
    "objectID": "LOESS.html#a-paraméterek-megválasztása",
    "href": "LOESS.html#a-paraméterek-megválasztása",
    "title": "1  A LOESS simító",
    "section": "1.8 A paraméterek megválasztása",
    "text": "1.8 A paraméterek megválasztása\nAdja magát a kérdés, hogy a paramétereket hogyan választhatjuk meg egy valódi helyzetben (értsd: ahol mi sem tudjuk mi az igazi függvény).\nItt most csak az érzékeltetés kedvéért mutatunk meg egy nagyon egyszerű módszert (megjegyezve, hogy ennél okosabban is el lehet járni, de ez is szemléltetni fogja, hogy a probléma kezelhető).\nAmit meg fogunk nézni az lényegében egy hold-out set validáció. A simitás jóságát azzal fogjuk mérni, hogy a simítógörbe és a pontok között mekkora a négyzetes eltérésösszeg. Ennek minimalizálása természetesen mindig alulsimított megoldást eredményezne, hiszen ez a célfüggvény nullába is vihető. Éppen ezért cselesebben járunk el: a pontokat véletlenszerűen két részre osztjuk, az egyik alapján határozzuk meg a simítógörbét (tanítóhalmaz), de a hibát a másik halmazon (teszthalmaz) mérjük le! Így ha elkezdünk túlsimítani, akkor a tanítóhalmazon ugyan csökken a hiba, de a teszthalmazon elkezd nőni. Azt a simítást választjuk tehát, ami a teszthalmazon mért hibát minimalizálja.\n\nSimData$train &lt;- FALSE\nSimData$train[sample(1:101, 80)] &lt;- TRUE\nggplot(SimData, aes(x = x, y = yobs, color = train)) + geom_point() +\n  geom_line(aes(y = y), color = \"orange\", lwd = 1)\n\n\n\n\n\n\n\n\nNézzük meg hogyan alakul a hiba a simítási paraméter változtatásával, ha az összes pontra illesztünk (az egyszerűség kedvéért a fokszám legyen fixen 1, tehát csak az \\(\\alpha\\) hatását vizsgáljuk – a fokszám, vagy bármilyen más paraméter ugyanígy lenne kezelhető):\n\nSmoothData2 &lt;- merge(SimData, CJ(x = unique(SimData$x), degree = 1,\n                                 span = (3:99)/100), by = \"x\")\nSmoothData2$value &lt;- apply(SmoothData2, 1, function(sm)\n  loessfun(sm[\"x\"], SimData$x, SimData$yobs, sm[\"span\"], sm[\"degree\"]))\n\nggplot(SmoothData2[, .(SSE = sum((value-yobs)^2)) , .(span)],\n       aes(x = span, y = SSE)) + geom_line()\n\n\n\n\n\n\n\n\nAhogy vártuk, a hiba folyamatosan csökken, az alulsimított megoldás tűnik a legjobbnak. Jól látható, hogy az alulsimítás a túlilleszkedés analóg fogalma!\nMost vessük be a trükköt: csak a tanítóhalmazra illesztünk, miközben a teszthalmazon mérjük a hibát. Íme az eredmény:\n\nSmoothData3 &lt;- merge(SimData[train==FALSE],\n                     CJ(x = unique(SimData[train==FALSE]$x), degree = 1,\n                        span = (3:99)/100), by = \"x\")\nSmoothData3$value &lt;- apply(SmoothData3, 1, function(sm)\n  loessfun(sm[\"x\"], SimData[train==TRUE]$x, SimData[train==TRUE]$yobs,\n           sm[\"span\"], sm[\"degree\"]))\nggplot(SmoothData3[, .(SSE = sum((value-yobs)^2)) , .(span)],\n       aes(x = span, y = SSE)) + geom_line()\n\n\n\n\n\n\n\n\nPontosan a várakozásainknak megfelelően így már szép, értelmes optimum van: mind a túl-, mind az alulsimítást észre tudjuk venni ezzel a validációval.\nAz optimális simítási paraméter értéke számszerűen is meghatározható:\n\noptspan &lt;- SmoothData3[, .(SSE = sum((value-yobs)^2)) , .(span)][order(SSE)][1]\noptspan\n\n    span       SSE\n   &lt;num&gt;     &lt;num&gt;\n1:  0.21 0.9498537\n\n\nA simítás ezzel:\n\np + geom_line(data = SmoothData[span==optspan$span&degree==1],\n              aes(x = x, y = value), color = \"red\")",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>A LOESS simító</span>"
    ]
  },
  {
    "objectID": "spline.html",
    "href": "spline.html",
    "title": "2  Spline fogalma, lineáris regressziótól a spline-regresszióig",
    "section": "",
    "text": "2.1 A regresszió\nA regresszió legtöbb alkalmazott statisztikai terület talán legfontosabb eszköze\nRegresszió: változók közti kapcsolat (illetve annak becslése minta alapján)\n„Kapcsolat” formalizálása: függvény a matematikai fogalmával, tehát keressük az\n\\[\nY=f\\left(X_1,X_2,\\ldots,X_p\\right)+\\varepsilon=f\\left(\\mathbf{X}\\right)\n\\]\nfüggvényt\n(\\(Y\\) eredményváltozó, \\(X_i\\)-k a magyarázó változók)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Spline fogalma, lineáris regressziótól a spline-regresszióig</span>"
    ]
  },
  {
    "objectID": "spline.html#regresszió-becslése-mintából",
    "href": "spline.html#regresszió-becslése-mintából",
    "title": "2  Spline fogalma, lineáris regressziótól a spline-regresszióig",
    "section": "2.2 Regresszió becslése mintából",
    "text": "2.2 Regresszió becslése mintából\nParaméteres regresszió: ha a priori feltételezzük, hogy az \\(f\\) függvény valamilyen – paraméterek erejéig meghatározott – függvényformájú (az „alakja” ismert), és így a feladat e paraméterek becslésére redukálódik\nTipikus példa a lineáris regresszió: \\(f\\left(\\mathbf{X}\\right)=\\beta_0+\\beta_1 X_1 + \\beta_2 X_2 + \\ldots + \\beta_p X_p=\\mathbf{X}^T\\pmb{\\beta}\\), így \\(Y=\\mathbf{X}^T\\pmb{\\beta}+\\varepsilon\\)\nHa rendelkezésre állnak az \\(\\left\\{y_i,\\mathbf{x}_i\\right\\}_{i=1}^n\\) megfigyeléseink a háttéreloszlásra, akkor e mintából megbecsülhetjük a paramétereket például hagyományos legkisebb négyzetek (OLS) módszerével:\n\\[\n\\widehat{\\pmb{\\beta}}=\\mathop{\\arg\\,\\min}_{\\mathbf{b}} \\sum_{i=1}^n \\left[Y_i-\\mathbf{X}_i^T\\mathbf{b}\\right]^2=\\left\\| \\mathbf{Y} - \\mathbf{X}\\mathbf{b} \\right\\|^2\n\\]\nItt tehát \\(\\mathbf{X}\\) az a mátrix, amiben a magyarázó változók elé egy csupa 1 oszlopot szúrtunk, a neve modellmátrix vagy design mátrix",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Spline fogalma, lineáris regressziótól a spline-regresszióig</span>"
    ]
  },
  {
    "objectID": "spline.html#paraméteres-és-nem-paraméteres-regresszió",
    "href": "spline.html#paraméteres-és-nem-paraméteres-regresszió",
    "title": "2  Spline fogalma, lineáris regressziótól a spline-regresszióig",
    "section": "2.3 Paraméteres és nem-paraméteres regresszió",
    "text": "2.3 Paraméteres és nem-paraméteres regresszió\nDe cserében mindig ott lebeg felettünk a kérdés, hogy a függvényformára jó feltételezést tettünk-e (hiszen ez nem az adatokból következik, ezt „ráerőszakoljuk” az adatokra)\n(Persze ezért van a modelldiagnosztika)\nA nem-paraméteres regresszió flexibilis, olyan értelemben, hogy minden a priori megkötés nélkül követi azt, ami az adatokból következik (a valóság ritkán lineáris?)\nCserében nehezebb becsülni, és nem kapunk analitikus – jó esetben valamire hasznosítható – regressziós függvényt, nem lehet értelmesen interpolálni és extrapolálni („fordul a kocka” a paraméteres esethez képest)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Spline fogalma, lineáris regressziótól a spline-regresszióig</span>"
    ]
  },
  {
    "objectID": "spline.html#a-lineáris-regresszió-kibővítése-nemlinearitások",
    "href": "spline.html#a-lineáris-regresszió-kibővítése-nemlinearitások",
    "title": "2  Spline fogalma, lineáris regressziótól a spline-regresszióig",
    "section": "2.4 A lineáris regresszió kibővítése, nemlinearitások",
    "text": "2.4 A lineáris regresszió kibővítése, nemlinearitások\nMaradva a paraméteres keretben, arra azért mód van, hogy a függvényformát kibővítsük (és így flexibilisebbé tegyük)\nEzzel a különféle nemlineáris regressziókhoz jutunk el\nE nemlinearitásoknak két alaptípusa van\n\nVáltozójában nemlineáris modell (pl. \\(\\beta_0 + \\beta_1 x + \\beta_2 x^2\\)): csak a szó „matematikai értelmében” nemlineáris, ugyanúgy becsülhető OLS-sel\nParaméterében nemlineáris modell (pl. \\(\\beta_0x_1^{\\beta_1}x_2^{\\beta_2}\\)): felrúgja a lineáris struktúrát, így érdemileg más, csak linearizálás után, vagy NLS-sel becsülhető\n\nMi most az első esettel fogunk foglalkozni\nAz itt látott „polinomiális regresszió” valóban nagyon gyakori módszer a flexibilitás növelésére",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Spline fogalma, lineáris regressziótól a spline-regresszióig</span>"
    ]
  },
  {
    "objectID": "spline.html#egy-példa",
    "href": "spline.html#egy-példa",
    "title": "2  Spline fogalma, lineáris regressziótól a spline-regresszióig",
    "section": "2.5 Egy példa",
    "text": "2.5 Egy példa\nTekintsünk most egy másik példát, egy zajos másodfokú függvényt, kevesebb pontból:\n\nn &lt;- 20\nx &lt;- runif(n, 0, 10)\nxgrid &lt;- seq(0, 10, length.out = 100)\nygrid &lt;- xgrid^2\nyobs &lt;- x^2 + rnorm(n, 0, 5)\nSimData &lt;- data.frame(x, xgrid, ygrid, yobs)\np &lt;- ggplot(SimData) + geom_point(aes(x = x, y = yobs)) +\n  geom_line(aes(x = xgrid, y = ygrid), color = \"orange\", lwd = 1)\np",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Spline fogalma, lineáris regressziótól a spline-regresszióig</span>"
    ]
  },
  {
    "objectID": "spline.html#regresszió-ötödfokú-polinommal",
    "href": "spline.html#regresszió-ötödfokú-polinommal",
    "title": "2  Spline fogalma, lineáris regressziótól a spline-regresszióig",
    "section": "2.6 Regresszió ötödfokú polinommal",
    "text": "2.6 Regresszió ötödfokú polinommal\n\nfit5 &lt;- lm(yobs ~ poly(x, 5), data = SimData)\np + geom_line(data = data.frame(xgrid, pred = predict(fit5, data.frame(x = xgrid))),\n              aes(x = xgrid, y = pred))",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Spline fogalma, lineáris regressziótól a spline-regresszióig</span>"
    ]
  },
  {
    "objectID": "spline.html#módosítás",
    "href": "spline.html#módosítás",
    "title": "2  Spline fogalma, lineáris regressziótól a spline-regresszióig",
    "section": "2.7 Módosítás",
    "text": "2.7 Módosítás\nMondjuk, hogy nagyobb flexibilitásra vágyunk\n\nPéldául figyelembe akarjuk venni, hogy ez nem tűnik teljesen lineárisnak, vagy meg akarjuk ragadni a finomabb tendenciákat is\n\nEmeljük a polinom fokszámát (ez nyilván növeli a flexibilitást, hiszen a kisebb fokszám nyilván speciális eset lesz), például 10-re\nSzokás azt mondani, hogy a rang 5 illetve 10 (a polinom fokszáma, a becsülendő paraméterek száma nyilván egyezik a modellmátrix rangjával, de ez a fogalom később, amikor nem is polinomunk van, akkor is használható)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Spline fogalma, lineáris regressziótól a spline-regresszióig</span>"
    ]
  },
  {
    "objectID": "spline.html#regresszió-tizedfokú-polinommal",
    "href": "spline.html#regresszió-tizedfokú-polinommal",
    "title": "2  Spline fogalma, lineáris regressziótól a spline-regresszióig",
    "section": "2.8 Regresszió tizedfokú polinommal",
    "text": "2.8 Regresszió tizedfokú polinommal\n\nfit10 &lt;- lm(yobs ~ poly(x, 10), data = SimData)\np + geom_line(data = data.frame(xgrid, pred = predict(fit10, data.frame(x = xgrid))),\n              aes(x = xgrid, y = pred))",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Spline fogalma, lineáris regressziótól a spline-regresszióig</span>"
    ]
  },
  {
    "objectID": "spline.html#mi-a-jelenség-oka",
    "href": "spline.html#mi-a-jelenség-oka",
    "title": "2  Spline fogalma, lineáris regressziótól a spline-regresszióig",
    "section": "2.9 Mi a jelenség oka?",
    "text": "2.9 Mi a jelenség oka?\nSzokás azt mondani, hogy túlilleszkedés, ami persze igaz is, de itt többről van szó\nA polinomok elsősorban lokálisan tudnak jól közelíteni (a Taylor-sorfejtéses érvelés miatt), de nekünk arra lenne szükségünk, hogy globálisan jól viselkedő függvényformát találjunk\nPedig a polinomokat amúgy szeretjük, többek között azért is, mert szép sima görbét írnak le (matematikai értelemben véve a simaságot: végtelenszer folytonosan deriválhatóak, \\(C^{\\infty}\\)-beliek)\nMi lehet akkor a megoldás?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Spline fogalma, lineáris regressziótól a spline-regresszióig</span>"
    ]
  },
  {
    "objectID": "spline.html#mi-lehet-a-megoldás",
    "href": "spline.html#mi-lehet-a-megoldás",
    "title": "2  Spline fogalma, lineáris regressziótól a spline-regresszióig",
    "section": "2.10 Mi lehet a megoldás?",
    "text": "2.10 Mi lehet a megoldás?\nEgy lehetséges megközelítés: „összerakjuk a globálisat több lokálisból”\nAzaz szakaszokra bontjuk a teljes intervallumot, és mindegyiket külön-külön polinommal igyekszünk modellezni\nÍgy próbáljuk kombinálni a két módszer előnyeit\nPersze a szakaszosan definiált polinomok önmagában még nem jók: a szakaszhatárokon találkozniuk kell (e találkozópontok neve: knot, „csomópont”, a számukat \\(q-2\\)-val jelöljük, a pozíciójukat \\(x_i^{\\ast}\\)-vel)\nSőt, ha a simasági tulajdonságokat is át akarjuk vinni, akkor az érintkezési pontokban a deriváltaknak (magasabbrendűeknek is) is egyezniük kell\nHa \\(p\\)-edfokú polinomokat használunk, akkor az első \\(p-1\\) derivált – és persze a függvényérték – egyezését kell kikötnünk a knot-okban (és esetleg még valamit a végpontokra)\nEz így már jó konstrukció lesz, a neve: spline",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Spline fogalma, lineáris regressziótól a spline-regresszióig</span>"
    ]
  },
  {
    "objectID": "spline.html#természetes-köbös-spline",
    "href": "spline.html#természetes-köbös-spline",
    "title": "2  Spline fogalma, lineáris regressziótól a spline-regresszióig",
    "section": "2.11 Természetes köbös spline",
    "text": "2.11 Természetes köbös spline\n(Azért köbös, mert harmadfokúak a polinomok, és azért természetes, mert azt kötöttük ki, hogy a végpontokban nulla legyen a második derivált)\n\n\n\nTermészetes köbös spline",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Spline fogalma, lineáris regressziótól a spline-regresszióig</span>"
    ]
  },
  {
    "objectID": "spline.html#a-példa-regressziója-természetes-köbös-spline-nal",
    "href": "spline.html#a-példa-regressziója-természetes-köbös-spline-nal",
    "title": "2  Spline fogalma, lineáris regressziótól a spline-regresszióig",
    "section": "2.12 A példa regressziója természetes köbös spline-nal",
    "text": "2.12 A példa regressziója természetes köbös spline-nal\n\nfitSpline &lt;- lm(yobs ~ splines::ns(x, 10), data = SimData)\np + geom_line(data = data.frame(xgrid, pred = predict(fitSpline, data.frame(x = xgrid))),\n              aes(x = xgrid, y = pred))",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Spline fogalma, lineáris regressziótól a spline-regresszióig</span>"
    ]
  },
  {
    "objectID": "spline.html#mi-az-előbbiben-a-fantasztikus",
    "href": "spline.html#mi-az-előbbiben-a-fantasztikus",
    "title": "2  Spline fogalma, lineáris regressziótól a spline-regresszióig",
    "section": "2.13 Mi az előbbiben a fantasztikus?",
    "text": "2.13 Mi az előbbiben a fantasztikus?\n\np + geom_line(data = rbind(data.frame(type = \"Ötödfokú polinom\",\n                                      pred = predict(fit5, data.frame(x = xgrid)), xgrid),\n                           data.frame(type = \"Tizedfokú polinom\",\n                                      pred = predict(fit10, data.frame(x = xgrid)), xgrid),\n                           data.frame(type = \"Spline\",\n                                      pred = predict(fitSpline, data.frame(x = xgrid)),\n                                      xgrid)),\n              aes(x = xgrid, y = pred, color = type)) + labs(color = \"\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Spline fogalma, lineáris regressziótól a spline-regresszióig</span>"
    ]
  },
  {
    "objectID": "spline.html#a-spline-regresszió-ereje",
    "href": "spline.html#a-spline-regresszió-ereje",
    "title": "2  Spline fogalma, lineáris regressziótól a spline-regresszióig",
    "section": "2.14 A spline-regresszió ereje",
    "text": "2.14 A spline-regresszió ereje\nNem csak az a jó, hogy szépen illeszkedik (tulajdonképpen még annál is jobban, mint a tizedfokú polinom, még ott is, ahol az jól illeszkedik amúgy)\n…hanem, hogy – most már elárulhatom – ez is ugyanúgy 10 rangú mint a tizedfokú polinom!\nMégis: nyoma nincs túlilleszkedésnek",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Spline fogalma, lineáris regressziótól a spline-regresszióig</span>"
    ]
  },
  {
    "objectID": "spline-regresszio.html",
    "href": "spline-regresszio.html",
    "title": "3  Spline-regresszió becslése bázisfüggvényekkel, penalizáltan",
    "section": "",
    "text": "3.1 Bázisfüggvényekkel felírás",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Spline-regresszió becslése bázisfüggvényekkel, penalizáltan</span>"
    ]
  },
  {
    "objectID": "spline-regresszio.html#bázisfüggvényekkel-felírás",
    "href": "spline-regresszio.html#bázisfüggvényekkel-felírás",
    "title": "3  Spline-regresszió becslése bázisfüggvényekkel, penalizáltan",
    "section": "",
    "text": "3.1.1 Hogyan becsüljük meg a spline-regressziót?\nAmiről nem beszéltünk eddig: ez mind szép, de hogyan tudunk ténylegesen is megbecsülni egy ilyen spline-regressziót?\nEhhez visszalépünk pár lépést, és bevezetünk egy első kicsit absztraktnak tűnő, de később rendkívül jó szolgálatot tevő megközelítést\nBár a célunk a spline-regresszió becslésének a megoldása, de a dolog – értelemszerűen – alkalmazható polinomiális regresszióra is (legfeljebb nincs sok értelme, mert az hagyományos módszerekkel is jól kézbentartható), úgyhogy először azon fogjuk illusztrálni\n\n\n3.1.2 Polinomok tere mint függvénytér\nA másodfokú polinomok – mint függvények – összessége függvényteret alkot\nEz egy olyan vektortér, aminek az elemei a függvények, a skalárok a valós számok, a két művelet pedig\n\nSkalárral szorzás: \\(\\left(cf\\right)\\left(x\\right)=cf\\left(x\\right)\\)\nVektorok (azaz függvények) összeadása: \\(\\left(f+g\\right)\\left(x\\right)=f\\left(x\\right)+g\\left(x\\right)\\), tehát pontonkénti összeadás\n\nBelátható, hogy ez teljesíti a vektortéraxiómákat, mert zárt a két műveletre (másodfokú polinomok összege másodfokú polinom és másodfokú polinom konstansszorosa másodfokú polinom), illetve az összeadásra nézve kommutatív csoport, a szorzás és az összeadás mindkét irányból disztributív, van egységelem szorzásra nézve és a skalárszorzás valamint a valós számok szorzása kompatibilis\n\n\n3.1.3 Polinomok terének bázisa\nSzuper, de mindez mire jó?\nHa vektortér, akkor létezik bázisa, azaz olyan vektorok halmaza, melyekből lineáris kombinációval minden vektor – egyértelműen – előállítható (bázis: lineárisan független generátorrendszer)\nA bázis nem feltétlenül egyértelmű, de az elemszáma igen, ez a vektortér dimenziója\nPéldául a másodfokú polinomok jó bázisa \\(\\left\\{1,x,x^2\\right\\}\\), nyilvánvaló, hogy ebből tényleg minden \\(ax^2+bx+c\\) másodfokú polinom előállítható lineáris kombinációval (triviálisan, a súlyok \\(c\\), \\(b\\) és \\(a\\))\nFüggvényterek esetében a bázis elemeit bázisfüggvényeknek is szokás nevezni, az \\(\\left\\{1,x,x^2\\right\\}\\) tehát a másodfokú polinomok bázisfüggvényei\n\n\n3.1.4 A polinomok terének dimenziója\nMivel mutattunk egy konkrét bázist, így a dimenzió nyilván 3, de a későbbiek szempontjából jól jön egy másik módszer is\nAzzal, hogy az \\(ax^2+bx+c\\) polinomot megfeleltettük az \\(\\left(a,b,c\\right)\\) valós számhármasnak, a polinomok tere és a valós számhármasok tere (az \\(\\mathbb{R}^3\\)) között létesítettünk egy izomorfizmust (a leképezés művelettartó és kölcsönösen egyértelmű)\nEmiatt a polinomok terének ugyanaz a dimenziója, mint az \\(\\mathbb{R}^3\\)-nak, ami viszont természetesen 3\nEz a módszer általában is használható: a dimenzió a felíráshoz szükséges paraméterek száma (feltéve, hogy ezek valós számok, valamint mindegyikhez tartozik egy polinom és viszont)\n\n\n3.1.5 Spline-ok függvénytere\nMindez a spline-okra is igaz!\nÉrthető: minden pontban két polinomot adunk össze, vagy polinomot szorzunk skalárral, az eredmény polinom (már láttuk) – így tud spline adott pontja lenni!\nAzaz: spline-okat is elő tudunk állítani bázisfüggvények lineáris kombinációjaként!\n\n\n3.1.6 Hány dimenziós a spline-ok tere?\nMielőtt megkeressük a spline-ok terének egy bázisát (azaz a konkrét bázisfüggvényeket), tisztázni kellene, hogy hány bázisfüggvényt keresünk egyáltalán, azaz hány dimenziós a spline-ok függvénytere\nNaiv ötlet (köbös spline-okat használva példaként): van \\(q-1\\) szakasz (\\(q-2\\) knot, ami meghatároz \\(q-3\\) szakaszt meg a két vége; úgy is felfogható, hogy a két végével együtt \\(q\\) knot van, ami meghatároz \\(q-1\\) szakaszt) és mindegyiken egy harmadfokú polinom (aminek 4 paramétere van), akkor az \\(4q-4\\) paraméter\nIgen ám, de vannak megkötések: a knotokban a függvényérték és az első két derivált egyezik\nMinden megkötés minden pontban 1 egyenlet, az 1-gyel csökkenti a paraméterek számát: van \\(q-2\\) knot és 3 megkötés, az \\(3q-6\\) csökkentés, marad \\(q+2\\) paraméter\nDe mivel természetes, így a végpontokban is van 1-1 megkötés: marad \\(q\\) paraméter, azaz \\(q\\) dimenziós a természetes köbös spline-ok tere (ezért neveztük a knot-ok számát \\(q-2\\)-nek!)\n\n\n3.1.7 Mik a spline-ok bázisfüggvényei?\nTermészetesen itt is igaz, hogy adott, rögzített spline-osztályra (pl. természetes köbös) is végtelen sok bázis van\nKöztük célszerűség alapján választhatunk\nA részletek nélkül két példa:\n\n\\(b_1\\left(x\\right)=1, b_2\\left(x\\right)=x, b_i\\left(x\\right)=\\left|x-x_{i-2}^{\\ast}\\right|^3 (i=3,4,\\ldots,q)\\)\n\\(b_1\\left(x\\right)=1, b_2\\left(x\\right)=x, b_i\\left(x\\right)=R\\left(x,x_{i-2}^{\\ast}\\right) (i=3,4,\\ldots,q)\\), ahol \\(R\\) egy nevezetes – elég hosszú, bár nem túl bonyolult – függvény (hamar látni fogjuk, hogy ez miért előnyös), annyi fontos, hogy \\(x\\) a \\(\\left[0,1\\right]\\) intervallumban essen (egyszerű átskálázással mindig elérhető)\n\nMost már csak a regresszió kivitelezését kell kitalálnunk",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Spline-regresszió becslése bázisfüggvényekkel, penalizáltan</span>"
    ]
  },
  {
    "objectID": "spline-regresszio.html#modellmátrix-előállítása",
    "href": "spline-regresszio.html#modellmátrix-előállítása",
    "title": "3  Spline-regresszió becslése bázisfüggvényekkel, penalizáltan",
    "section": "3.2 Modellmátrix előállítása",
    "text": "3.2 Modellmátrix előállítása\n\n3.2.1 A bázisfüggvények használatának ereje\nA bázisfüggvények használatának két hatalmas előnye van:\n\nA probléma visszavezethető velük a sima lineáris regresszióra\nSőt, ehhez a modellmátrix is könnyen előállítható\n\n\n\n3.2.2 Bázisfüggvények használata másodfokú polinomnál\nLegyen \\(b_1\\left(x\\right)=1\\), \\(b_2\\left(x\\right)=x\\) és \\(b_3\\left(x\\right)=x^2\\) a bázisunk\nAz eredeti regresszió:\n\\[\ny_i = \\beta_1 + \\beta_2 x_i + \\beta_3 x_i^2 + \\varepsilon_i\n\\]\nÁtírva bázisokra (lényegében transzformált magyarázó változók):\n\\[\ny_i = \\beta_1 b_1\\left(x_i\\right) + \\beta_2 b_2\\left(x_i\\right) + \\beta_3 b_3\\left(x_i\\right) + \\varepsilon_i\n\\]\nEz már tiszta lineáris regresszió\n\n\n3.2.3 Bázisfüggvények használatának előnye\nEz úgy tűnik, hogy csak egy nagyon nyakatekert felírás egy amúgy egyszerű problémára\nValójában viszont egy elképesztően erőteljes dolgot nyertünk: minden olyan függvény, legyen bármilyen komplikált is, ami felírható bázisfüggvényekkel (azaz az osztálya függvényosztályt alkot), az berakható egy kutyaközönséges regresszióba (azaz lehet ő a regrssziós függvény) a fenti transzformációval, tehát\n\\[\n    \\sum_{i=1}^q \\beta_i b_i\\left(x\\right)\n\\]\nalakban\n(Azaz minden függvény, ami egy függvénytér eleme)\n\n\n3.2.4 A bázisfüggvények ereje, 1. felvonás\nMég egyszer: minden függvény, ami felírható bázisfüggvényekkel\nAzaz: minden\n…és az összesnek pontosan ugyanúgy az lesz az alakja, hogy\n\\[\n\\sum_{i=1}^q \\beta_i b_i\\left(x\\right),\n\\]\negyedül a bázisfüggvényt kell az adott esetnek megfelelően megválasztani\nTehát a spline is mehet ugyanígy (csak megfelelő \\(b_i\\)-kkel)!\nÉs ha ez az alak megvan, akkor onnantól természetesen sima lineáris regresszióval elintézhető\n\n\n3.2.5 A bázisfüggvények ereje, 2. felvonás\nRáadásul az \\(\\mathbf{X}\\) modellmátrix (design mátrix) előállítása is nagyon könnyű lesz: az \\(i\\)-edik sora\n\\[\n\\left[ b_1\\left(x_i\\right), b_2\\left(x_i\\right), \\ldots, b_q\\left(x_i\\right) \\right]\n\\]\nÍgy maga a mátrix az \\(\\mathbf{x}\\) és az \\(\\left[1,2,\\ldots,q\\right]\\) vektor külső szorzata (tenzorszorzata), ha a művelet alatt az oszlopban szereplő érték által meghatározott bázisfüggvény sorbeli elemre történő alkalmazását értjük, tehát \\(i\\otimes j:=b_j\\left(x_i\\right)\\), és így\n\\[\n\\begin{aligned}\n& \\begin{pmatrix} \\quad 1 & \\qquad \\enspace 2 & \\quad \\; \\cdots & \\quad q\\quad \\; \\end{pmatrix} \\\\\n\\begin{pmatrix}x_1\\\\x_2\\\\ \\vdots \\\\ x_n\\end{pmatrix} & \\begin{bmatrix}b_1\\left(x_1\\right) & b_2\\left(x_1\\right) & \\cdots & b_q\\left(x_1\\right) \\\\ b_1\\left(x_2\\right) & b_2\\left(x_2\\right) & \\cdots & b_q\\left(x_2\\right) \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ b_1\\left(x_n\\right) & b_2\\left(x_n\\right) & \\cdots & b_q\\left(x_n\\right) \\end{bmatrix}\n\\end{aligned}\n\\]\nÍgy, a teljes modellmátrix egy lépésben megkapható…\n… majd közvetlenül rakható is bele a sima lineáris regresszióba (ld. 1. előny):\n\\[\n\\widehat{\\boldsymbol{\\beta}}=\\left(\\mathbf{X}^T\\mathbf{X}\\right)^{-1}\\mathbf{X}^T\\mathbf{y}\n\\]\n\n\n3.2.6 Megvalósítás R alatt\nFolytassuk az előző fejezet példáját, csak az egyszerűség kedvéért a \\(\\left[0,1\\right]\\) intervallumon lévő \\(x\\)-szel (ha nem is így lenne, ez átskálázással mindig elérhető):\n\nn &lt;- 30\nx &lt;- runif(n, 0, 1)\nxgrid &lt;- seq(0, 1, length.out = 100)\nygrid &lt;- 100*xgrid^2\nyobs &lt;- 100*x^2 + rnorm(n, 0, 5)\np &lt;- ggplot(data.frame(x, yobs)) + geom_point(aes(x = x, y = yobs)) +\n    geom_line(data = data.frame(xgrid, ygrid), aes(x = xgrid, y = ygrid),\n              color = \"orange\", lwd = 1)\np\n\n\n\n\n\n\n\n\nA csomópontokat egyenletesen vesszük fel, számuk \\(q-2\\):\n\nxk &lt;- 1:4/5\nq &lt;- length(xk) + 2\n\nA bázisfüggvényeknél említett \\(R\\) függvény:\n\nrk &lt;- function( x, z ) {\n    ((z-0.5)^2-1/12)*((x-0.5)^2-1/12)/4-((abs(x-z)-0.5)^4-(abs(x-z)-0.5)^2/2+7/240)/24\n}\n\nA modellmátrixot csupa 1-gyel inicializáljuk, így az első oszlop rendben is lesz:\n\nX &lt;- matrix(1, n, q) \n\nBeállítjuk a második oszlopot is:\n\nX[, 2] &lt;- x\n\nÉs most jön a trükk: az outer tetszőleges függvénnyel tud „külső szorzatot” képezni:\n\nX[, 3:q] &lt;- outer(x, xk, FUN = rk)\n\nMindezeket a késsőbbiekre tekintettel egy függvénybe is összefoghatjuk:\n\nspl.X &lt;- function(x, xk) {\n    q &lt;- length(xk) + 2\n    n &lt;- length(x)\n    X &lt;- matrix(1, n, q)\n    X[, 2] &lt;- x\n    X[, 3:q] &lt;- outer(x, xk, FUN = rk)\n    X\n}\n\nEzzel a modellmátrixszal végrehajthatjuk a regressziót (ne felejtsük, tengelymetszetre nincs szükség, pontosabban külön tengelymetszre nincs, hiszen az már benne van az így összerakott X-ben):\n\nfit &lt;- lm(yobs ~ X - 1 )\n\nAz eredmény szemléltetéséhez az xgrid pontjait is kifejtjük a spline-nal:\n\nXp &lt;- spl.X(xgrid, xk)\nyp &lt;- Xp%*%coef(fit)\np + geom_line(data = data.frame(xgrid, yp), aes(x = xgrid, y = yp))\n\n\n\n\n\n\n\n\nMég egy kicsit automatizáljunk:\n\npredspline &lt;- function(x, y, q) {\n    xk &lt;- (1:(q-2))/(q-1)\n    X &lt;- spl.X(x, xk)\n    fit &lt;- lm(y ~ X - 1)\n    xp &lt;- 0:100/100\n    Xp &lt;- spl.X(xp, xk)\n    yp &lt;- Xp%*%coef(fit)\n    list(fit = fit, xp = xp, yp = yp)\n}\n\nÍgy például könnyen megnézhetjük az eredményt különböző \\(q\\)-kkal:\n\np + geom_line(data = with(predspline(x, yobs, 6), data.frame(xp, yp)),\n              aes(x = xp, y = yp))\n\n\n\n\n\n\n\np + geom_line(data = with(predspline(x, yobs, 11), data.frame(xp, yp)),\n              aes(x = xp, y = yp))\n\n\n\n\n\n\n\np + geom_line(data = with(predspline(x, yobs, 3), data.frame(xp, yp)),\n              aes(x = xp, y = yp))\n\n\n\n\n\n\n\n\nLátszik, hogy a \\(q=6\\) nagyjából megfelelő, a 11 kicsit sok, a 3 pedig egy leheletnyit mintha kevés lenne. (Most persze könnyű dolgunk van, hiszen tudjuk mi az igazság!) Erre a kérdésre nemsokára visszatérünk.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Spline-regresszió becslése bázisfüggvényekkel, penalizáltan</span>"
    ]
  },
  {
    "objectID": "spline-regresszio.html#penalizálás",
    "href": "spline-regresszio.html#penalizálás",
    "title": "3  Spline-regresszió becslése bázisfüggvényekkel, penalizáltan",
    "section": "3.3 Penalizálás",
    "text": "3.3 Penalizálás\n\n3.3.1 Dimenzió meghatározása\nA \\(q\\) dimenzió tehát az illeszkedés szabadságát határozza meg\nValahogy ezt is meg kellene határozni\nJön a fő kérdéskör: a túlilleszkedés elleni védekezés\nMilyen legyen a „simítás foka”?\n\n\n3.3.2 Simítás fokának meghatározása\nTehát \\(q\\)-t kellene valahogy jól belőni\nEgyszerű modellszelekció?\n\nVagy nem beágyazott modellek szelekciója, vagy nem ekvidisztáns knot-ok, egyik sem túl szerencsés\n\nAlternatív ötlet: \\(q\\) legyen inkább rögzített (elég nagy értéken, kicsit a várható fölé lőve), de a függvényformát nem engedjük teljesen szabadon alakulni\nHogyan? Büntetjük a túl „zizegős” függvényt!\nEz épp a penalizált regresszió alapötlete\nÉs ami rendkívül fontos: így már jellemzően sem \\(q\\) pontos megválasztása, sem a knot-ok pontos helye nem bír nagy jelentőséggel (választhatjuk például egyenletesen)!\n\n\n3.3.3 Penalizált regresszió\nKlasszikus megoldás: a második derivált jelzi adott pontban a „zizegősséget”, ezt kiintegrálva kapunk egy összesített mértéket az egész függvényre\nValamilyen súllyal ezt vegyük figyelembe:\n\\[\n    \\left\\|\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\beta}\\right\\|^2+\\lambda\\int_0^1 \\left[f''\\left(x\\right)\\right]^2 \\mathop{}\\!{\\mathrm{d}} x\n\\]\nA \\(\\lambda\\) a simítási paraméter, ez határozza meg a trade-off-ot a jó illeszkedés és a simaság között\n\n\\(\\lambda=0\\): penalizálatlan becslés, \\(\\lambda\\rightarrow\\infty\\): egyenes regressziós függvény\n\n\n\n3.3.4 A simasági büntetőtag meghatározása\nA regressziós függvény alakja: \\(f\\left(x\\right)=\\sum_{i=1}^q \\beta_i b_i\\left(x\\right)\\)\nKétszer deriválva: \\(f''\\left(x\\right)=\\sum_{i=1}^q \\beta_i b_i''\\left(x\\right)\\)\nNégyzetre emelve: \\(\\left[f''\\left(x\\right)\\right]^2=\\sum_{i=1}^q \\sum_{j=1}^q \\beta_i b_i''\\left(x\\right) b_j''\\left(x\\right) \\beta_j\\)\nKiintegrálva: \\(\\int_0^1 \\left[f''\\left(x\\right)\\right]^2 \\mathop{}\\!{\\mathrm{d}} x=\\sum_{i=1}^q \\sum_{j=1}^q \\beta_i \\left(\\int_0^1 b_i''\\left(x\\right) b_j''\\left(x\\right) \\mathop{}\\!{\\mathrm{d}} x\\right) \\beta_j\\)\nDe hát ez épp egy kvadratikus alak! (\\(\\sum_{i=1}^q \\sum_{j=1}^q x_i a_{ij} x_j= \\mathbf{x}^T \\mathbf{A} \\mathbf{x}\\))\nLegyen \\(S_{ij}=\\int_0^1 b_i''\\left(x\\right) b_j''\\left(x\\right) \\mathop{}\\!{\\mathrm{d}} x\\) és \\(\\mathbf{S}\\) az ezekből alkotott mátrix, akkor tehát a simítási büntetőtag:\n\\[\n    \\lambda \\boldsymbol{\\beta}^T\\mathbf{S}\\boldsymbol{\\beta}\n\\]\nAz előbb definiált \\(R\\)-rel \\(\\mathbf{S}\\) alakja nagyon egyszerű lesz: \\(S_{i+2,j+2}=R\\left(x_i^{\\ast},x_j^{\\ast}\\right)\\), az első két oszlop és sor pedig csupa nulla\n\n\n3.3.5 Megvalósítás R alatt\nAz xk szokásosan a knot-ok helye; a mátrixot pedig csupa nullával inicializáljuk, hogy az első két oszlop és sor egyből jó is legyen és csak a többit kelljen kitölteni:\n\nspl.S &lt;- function(xk) {\n    q &lt;- length(xk) + 2\n    S &lt;- matrix(0, q, q)\n    S[3:q, 3:q] &lt;- outer(xk, xk, FUN = rk)\n    S\n}\n\n\n\n3.3.6 A simítási büntetőtag beépítése a regressziós célfüggvénybe\nKényelmes lenne, ha \\(\\left\\|\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\beta}\\right\\|^2+\\lambda \\boldsymbol{\\beta}^T\\mathbf{S}\\boldsymbol{\\beta}\\) helyett írhatnánk egyetlen normát célfüggvényként\nEz nem nehéz, ha a második tagot át tudjuk normává alakítani, hiszen (innentől némi blokkmátrix műveletekre szükség lesz)\n\\[\n\\left\\|\\mathbf{a}\\right\\|^2+\\left\\|\\mathbf{b}\\right\\|^2=\\left\\|\\begin{pmatrix}\\mathbf{a} \\\\ \\mathbf{b} \\end{pmatrix}\\right\\|^2\n\\]\nLegyen \\(\\mathbf{B}\\) olyan, hogy \\(\\mathbf{B}^T\\mathbf{B}=\\mathbf{S}\\) (pl. spektrális dekompozícióval, vagy Cholesky-dekompozícióval megtalálható a mátrix ilyen „négyzetgyöke”), ekkor\n\\[\n\\lambda \\boldsymbol{\\beta}^T\\mathbf{S}\\boldsymbol{\\beta} = \\lambda \\boldsymbol{\\beta}^T\\mathbf{B}^T\\mathbf{B}\\boldsymbol{\\beta}=\\lambda \\left( \\mathbf{B} \\boldsymbol{\\beta}\\right)^T\\mathbf{B}\\boldsymbol{\\beta} =\\left( \\sqrt{\\lambda} \\mathbf{B} \\boldsymbol{\\beta}\\right)^T\\left(\\sqrt{\\lambda}\\mathbf{B}\\boldsymbol{\\beta}\\right)\n\\]\nEzzel meg is vagyunk, hiszen a norma egyszerűen \\(\\left\\|\\mathbf{a}\\right\\|^2=\\mathbf{a}^T\\mathbf{a}\\), így\n\\[\n\\lambda \\boldsymbol{\\beta}^T\\mathbf{S}\\boldsymbol{\\beta} = \\left\\|\\sqrt{\\lambda} \\mathbf{B} \\boldsymbol{\\beta}\\right\\|^2\n\\]\nahonnan\n\\[\n\\left\\|\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\beta}\\right\\|^2+\\lambda \\boldsymbol{\\beta}^T\\mathbf{S}\\boldsymbol{\\beta}=\\left\\|\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\beta}\\right\\|^2+\\left\\|\\sqrt{\\lambda} \\mathbf{B} \\boldsymbol{\\beta}\\right\\|^2\n\\]\nés így, az előzőek szerint\n\\[\n\\left\\|\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\beta}\\right\\|^2+\\left\\|\\sqrt{\\lambda} \\mathbf{B} \\boldsymbol{\\beta}\\right\\|^2=\\left\\|\\begin{pmatrix}\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\beta} \\\\ \\sqrt{\\lambda} \\mathbf{B} \\boldsymbol{\\beta} \\end{pmatrix}\\right\\|^2\n\\]\nJó lenne \\(\\boldsymbol{\\beta}\\)-t kiemelni; ez nem is túl nehéz, hiszen \\(\\mathbf{a}\\) és \\(-\\mathbf{a}\\) normája ugyanaz:\n\\[\n\\left\\|\\begin{pmatrix}\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\beta} \\\\ \\sqrt{\\lambda} \\mathbf{B} \\boldsymbol{\\beta} \\end{pmatrix}\\right\\|^2 = \\left\\| \\begin{pmatrix} \\mathbf{y} \\\\ \\mathbf{0} \\end{pmatrix} - \\begin{pmatrix}\\mathbf{X} \\\\ \\sqrt{\\lambda} \\mathbf{B} \\end{pmatrix}\\boldsymbol{\\beta}\\right\\|^2\n\\]\n\n\n3.3.7 Regresszió megoldása a penalizálással\nInnentől a regresszió játszi könnyedséggel (értsd: a szokványos, nem is penalizált eszköztárral) megoldható, csak \\(\\mathbf{X}\\) szerepét \\(\\begin{pmatrix}\\mathbf{X} \\\\ \\sqrt{\\lambda} \\mathbf{B} \\end{pmatrix}\\), \\(\\mathbf{y}\\) szerepét \\(\\begin{pmatrix} \\mathbf{y} \\\\ \\mathbf{0} \\end{pmatrix}\\) játssza\nÍgy az „\\(\\mathbf{X}^T\\mathbf{X}\\)” épp \\(\\mathbf{X}^T\\mathbf{X}+\\lambda \\mathbf{B}^T\\mathbf{B}=\\mathbf{X}^T\\mathbf{X}+\\lambda\\mathbf{S}\\) lesz\nAz „\\(\\mathbf{X}^T\\mathbf{y}\\)” pedig \\(\\mathbf{X}^T\\mathbf{y}\\) (a kiegészített eredményváltozóban lévő nullák épp a magyarázó változók kiegészítését ütik ki)\nÍgy az OLS megoldás:\n\\[\n\\widehat{\\boldsymbol{\\beta}}=\\left(\\mathbf{X}^T\\mathbf{X}+\\lambda\\mathbf{S}\\right)^{-1}\\mathbf{X}^T\\mathbf{y}\n\\]\n(Persze a gyakorlatban ennek közvetlen számítása helyett célszerűbb az augmentált eredmény- és magyarázóváltozókat berakni egy hatékonyabb lineáris regressziót megoldó módszerbe)\n\n\n3.3.8 Megvalósítás R alatt\nMátrix „gyökének” a számítása (spektrális felbontással):\n\nmat.sqrt &lt;- function(S) {\n    d &lt;- eigen(S, symmetric = TRUE)\n    d$vectors%*%diag(d$values^0.5)%*%t(d$vectors)\n}\n\nAhogy volt róla, penalizálás mellett a \\(q\\) pontos értéke nem túl fontos, csak ne legyen túl kicsi, ezért használjunk most \\(q=20\\)-at.\nA penalizált becslés az augmentált modellmátrix használatával (kihasználjuk, hogy ha nem létező elemre hivatkozunk, az R automatikusan kiegészíti a vektort):\n\npredsplinepen &lt;- function(x, y, q, lambda) {\n    xk &lt;- (1:(q-2))/(q-1)\n    Xa &lt;- rbind(spl.X(x, xk), sqrt(lambda) * mat.sqrt(spl.S(xk)))\n    ya &lt;- c(y, rep(0, q))\n    fit &lt;- lm(ya ~ Xa - 1)\n    xp &lt;- 0:100/100\n    Xp &lt;- spl.X(xp, xk)\n    yp &lt;- Xp%*%coef(fit)\n    list(fit = fit, xp = xp, yp = yp)\n}\n\nEzzel könnyen meghatározhatjuk az eredményt különböző \\(\\lambda\\)-kra:\n\np + geom_line(data = with(predsplinepen(x, yobs, 20, 1), data.frame(xp, yp)),\n              aes(x = xp, y = yp))\n\n\n\n\n\n\n\np + geom_line(data = with(predsplinepen(x, yobs, 20, 0.001), data.frame(xp, yp)),\n              aes(x = xp, y = yp))\n\n\n\n\n\n\n\np + geom_line(data = with(predsplinepen(x, yobs, 20, 0.000001), data.frame(xp, yp)),\n              aes(x = xp, y = yp))\n\n\n\n\n\n\n\n\nLátható, hogy a \\(\\lambda=1\\) túl nagy, a \\(0,\\!001\\) jónak tűnik, a \\(0,\\!000001\\) túl kicsi.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Spline-regresszió becslése bázisfüggvényekkel, penalizáltan</span>"
    ]
  },
  {
    "objectID": "spline-regresszio.html#simítási-paraméter-meghatározása",
    "href": "spline-regresszio.html#simítási-paraméter-meghatározása",
    "title": "3  Spline-regresszió becslése bázisfüggvényekkel, penalizáltan",
    "section": "3.4 Simítási paraméter meghatározása",
    "text": "3.4 Simítási paraméter meghatározása\n\n3.4.1 A simítási paraméter meghatározása\nKérdés még a \\(\\lambda\\) értéke\nSima OLS-jellegű eljárással, tehát a reziduális négyzetösszeg minimalizálást tűzve ki célul nyilván nem határozható meg (hiszen az mindig 0-t adna)\nÉpp az a lényeg, hogy a túlilleszkedésre is tekintettel legyünk\nÖtlet: keresztvalidáció\n\n\n3.4.2 Keresztvalidációs módszerek: OCV\nMindig egy pontot hagyunk ki, és így számolunk hibát: OCV\n(Szokták egy-kihagyásos keresztvalidációnak, LOOCV-nek is nevezni)\nTehát:\n\\[\n    E_{OCV}=\\frac{1}{n}\\sum_{i=1}^n \\left( \\widehat{f}_i^{\\left[-i\\right]} - y_i\\right)^2\n\\]\nSzerencsére nem kell ténylegesen \\(n\\)-szer lefuttatni a regressziót mert belátható, hogy\n\\[\n    E_{OCV}=\\frac{1}{n}\\sum_{i=1}^n \\left(y_i - \\widehat{f}_i\\right)^2/\\left(1-A_{ii}\\right)^2,\n\\]\nahol \\(\\mathbf{A}\\) az influence mátrix\n\n\n3.4.3 Keresztvalidációs módszerek: GCV\nHa az \\(A_{ii}\\)-ket az átlagukkal helyettesítjük, akkor az általánosított keresztvalidációhoz jutunk (GCV)\nTehát:\n\\[\n    E_{GCV}=\\frac{1}{n}\\sum_{i=1}^n \\left(y_i - \\widehat{f}_i\\right)^2/\\left[\\mathrm{tr}\\left(\\mathbf{I}-\\mathbf{A}\\right)\\right]^2\n\\]\n\n\n3.4.4 Megvalósítás R alatt\n\npredV &lt;- 10^(seq(-8, 3, length.out = 100))\nV &lt;- sapply(predV, function(lambda) {\n    fit &lt;- predsplinepen(x, yobs, 20, lambda)$fit\n    trA &lt;- sum(influence(fit)$hat[1:n])\n    rss &lt;- sum((yobs - fitted(fit)[1:n])^2)\n    n*rss/(n - trA)^2\n} )\nggplot(data.frame(predV, V), aes(x = predV, y = V)) + geom_line() + scale_x_log10()\n\n\n\n\n\n\n\n\nA legjobb \\(\\lambda\\) konkrét érték:\n\npredV[which.min(V)]\n\n[1] 0.0003593814\n\n\nÉs az – ilyen értelemben – optimális spline ezzel:\n\np + geom_line(data = with(predsplinepen(x, yobs, 20, predV[which.min(V)]),\n                          data.frame(xp, yp)), aes(x = xp, y = yp))",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Spline-regresszió becslése bázisfüggvényekkel, penalizáltan</span>"
    ]
  },
  {
    "objectID": "additiv-modell.html",
    "href": "additiv-modell.html",
    "title": "4  Additív modellek",
    "section": "",
    "text": "4.1 Több magyarázó változó\nEddig egy magyarázó változó esetével foglalkoztunk",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Additív modellek</span>"
    ]
  }
]